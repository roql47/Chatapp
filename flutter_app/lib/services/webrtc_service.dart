import 'package:flutter_webrtc/flutter_webrtc.dart';
import '../config/app_config.dart';

class WebRTCService {
  RTCPeerConnection? _peerConnection;
  MediaStream? _localStream;
  MediaStream? _remoteStream;
  
  final RTCVideoRenderer localRenderer = RTCVideoRenderer();
  final RTCVideoRenderer remoteRenderer = RTCVideoRenderer();

  // ì½œë°±
  Function(MediaStream)? onLocalStream;
  Function(MediaStream)? onRemoteStream;
  Function(RTCIceCandidate)? onIceCandidate;
  Function(RTCSessionDescription)? onOffer;
  Function(RTCSessionDescription)? onAnswer;

  bool _isInitialized = false;
  bool _isSpeakerOn = true;
  bool get isInitialized => _isInitialized;

  // ì´ˆê¸°í™”
  Future<void> initialize() async {
    await localRenderer.initialize();
    await remoteRenderer.initialize();
    _isInitialized = true;
    
    // ìŠ¤í”¼ì»¤í° ê¸°ë³¸ í™œì„±í™”
    await Helper.setSpeakerphoneOn(true);
  }

  // ë¡œì»¬ ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ì‹œì‘
  Future<void> startLocalStream({bool video = true, bool audio = true}) async {
    final Map<String, dynamic> mediaConstraints = {
      'audio': audio ? {
        'echoCancellation': true,
        'noiseSuppression': true,
        'autoGainControl': true,
      } : false,
      'video': video ? {
        'facingMode': 'user',
        'width': {'ideal': 1280},
        'height': {'ideal': 720},
      } : false,
    };

    try {
      _localStream = await navigator.mediaDevices.getUserMedia(mediaConstraints);
      localRenderer.srcObject = _localStream;
      
      // ë¡œì»¬ ì˜¤ë””ì˜¤ íŠ¸ë™ í™œì„±í™” í™•ì¸
      for (var track in _localStream!.getAudioTracks()) {
        track.enabled = true;
        print('ğŸ¤ ë¡œì»¬ ì˜¤ë””ì˜¤ íŠ¸ë™ í™œì„±í™”: ${track.id}');
      }
      
      // ìŠ¤í”¼ì»¤í° í™œì„±í™”
      await Helper.setSpeakerphoneOn(_isSpeakerOn);
      
      onLocalStream?.call(_localStream!);
    } catch (e) {
      print('Error getting user media: $e');
      rethrow;
    }
  }

  // PeerConnection ìƒì„±
  Future<void> initPeerConnection() async {
    final configuration = <String, dynamic>{
      'iceServers': AppConfig.iceServers,
    };
    
    _peerConnection = await createPeerConnection(configuration);

    // ë¡œì»¬ ìŠ¤íŠ¸ë¦¼ ì¶”ê°€
    if (_localStream != null) {
      _localStream!.getTracks().forEach((track) {
        _peerConnection?.addTrack(track, _localStream!);
      });
    }

    // ICE Candidate ì´ë²¤íŠ¸
    _peerConnection?.onIceCandidate = (candidate) {
      if (candidate.candidate != null) {
        onIceCandidate?.call(candidate);
      }
    };

    // ì›ê²© ìŠ¤íŠ¸ë¦¼ ìˆ˜ì‹ 
    _peerConnection?.onTrack = (event) {
      print('ğŸ§ ì›ê²© íŠ¸ë™ ìˆ˜ì‹ : ${event.track.kind}, enabled: ${event.track.enabled}');
      if (event.streams.isNotEmpty) {
        _remoteStream = event.streams[0];
        remoteRenderer.srcObject = _remoteStream;
        
        // ì˜¤ë””ì˜¤ íŠ¸ë™ í™œì„±í™” í™•ì¸
        final audioTracks = _remoteStream!.getAudioTracks();
        print('ğŸ”Š ì›ê²© ì˜¤ë””ì˜¤ íŠ¸ë™ ìˆ˜: ${audioTracks.length}');
        for (var track in audioTracks) {
          track.enabled = true;
          print('ğŸ”Š ì›ê²© ì˜¤ë””ì˜¤ íŠ¸ë™ í™œì„±í™”: ${track.id}, enabled: ${track.enabled}');
        }
        
        // ë¹„ë””ì˜¤ íŠ¸ë™ í™•ì¸
        final videoTracks = _remoteStream!.getVideoTracks();
        print('ğŸ“¹ ì›ê²© ë¹„ë””ì˜¤ íŠ¸ë™ ìˆ˜: ${videoTracks.length}');
        for (var track in videoTracks) {
          print('ğŸ“¹ ì›ê²© ë¹„ë””ì˜¤ íŠ¸ë™: ${track.id}, enabled: ${track.enabled}');
        }
        
        // ìŠ¤í”¼ì»¤í° ê°•ì œ í™œì„±í™”
        _isSpeakerOn = true;
        Helper.setSpeakerphoneOn(true);
        print('ğŸ”Š ìŠ¤í”¼ì»¤í° ê°•ì œ í™œì„±í™”');
        
        onRemoteStream?.call(_remoteStream!);
      } else {
        print('âš ï¸ ì›ê²© ìŠ¤íŠ¸ë¦¼ì´ ë¹„ì–´ìˆìŒ');
      }
    };

    // ì—°ê²° ìƒíƒœ ë³€ê²½
    _peerConnection?.onConnectionState = (state) {
      print('ğŸ”— Connection state: $state');
      if (state == RTCPeerConnectionState.RTCPeerConnectionStateConnected) {
        print('âœ… WebRTC ì—°ê²° ì™„ë£Œ! ìŠ¤í”¼ì»¤í° ì¬í™•ì¸');
        Helper.setSpeakerphoneOn(true);
      }
    };

    _peerConnection?.onIceConnectionState = (state) {
      print('ğŸ§Š ICE connection state: $state');
      if (state == RTCIceConnectionState.RTCIceConnectionStateConnected) {
        print('âœ… ICE ì—°ê²° ì™„ë£Œ!');
        // ì—°ê²° ì™„ë£Œ ì‹œ ìŠ¤í”¼ì»¤í° ì¬í™œì„±í™”
        Helper.setSpeakerphoneOn(true);
      }
    };
    
    // ì‹œê·¸ë„ë§ ìƒíƒœ ë³€ê²½
    _peerConnection?.onSignalingState = (state) {
      print('ğŸ“¡ Signaling state: $state');
    };
  }

  // Offer ìƒì„±
  Future<RTCSessionDescription> createOffer() async {
    final offer = await _peerConnection!.createOffer({
      'offerToReceiveVideo': true,
      'offerToReceiveAudio': true,
    });
    await _peerConnection!.setLocalDescription(offer);
    return offer;
  }

  // Answer ìƒì„±
  Future<RTCSessionDescription> createAnswer() async {
    final answer = await _peerConnection!.createAnswer({
      'offerToReceiveVideo': true,
      'offerToReceiveAudio': true,
    });
    await _peerConnection!.setLocalDescription(answer);
    return answer;
  }

  // Remote Description ì„¤ì •
  Future<void> setRemoteDescription(RTCSessionDescription description) async {
    await _peerConnection?.setRemoteDescription(description);
  }

  // ICE Candidate ì¶”ê°€
  Future<void> addIceCandidate(RTCIceCandidate candidate) async {
    await _peerConnection?.addCandidate(candidate);
  }

  // ì¹´ë©”ë¼ ì „í™˜
  Future<void> switchCamera() async {
    if (_localStream != null) {
      final videoTrack = _localStream!.getVideoTracks().first;
      await Helper.switchCamera(videoTrack);
    }
  }

  // ë§ˆì´í¬ ìŒì†Œê±° í† ê¸€
  void toggleMute() {
    if (_localStream != null) {
      final audioTrack = _localStream!.getAudioTracks().first;
      audioTrack.enabled = !audioTrack.enabled;
    }
  }

  // ë¹„ë””ì˜¤ í† ê¸€
  void toggleVideo() {
    if (_localStream != null) {
      final videoTrack = _localStream!.getVideoTracks().first;
      videoTrack.enabled = !videoTrack.enabled;
    }
  }

  // ë§ˆì´í¬ ìŒì†Œê±° ìƒíƒœ
  bool get isMuted {
    if (_localStream != null && _localStream!.getAudioTracks().isNotEmpty) {
      return !_localStream!.getAudioTracks().first.enabled;
    }
    return false;
  }

  // ë¹„ë””ì˜¤ êº¼ì§ ìƒíƒœ
  bool get isVideoOff {
    if (_localStream != null && _localStream!.getVideoTracks().isNotEmpty) {
      return !_localStream!.getVideoTracks().first.enabled;
    }
    return true;
  }
  
  // ìŠ¤í”¼ì»¤ í† ê¸€
  Future<void> toggleSpeaker() async {
    _isSpeakerOn = !_isSpeakerOn;
    await Helper.setSpeakerphoneOn(_isSpeakerOn);
    print('ğŸ”Š ìŠ¤í”¼ì»¤: ${_isSpeakerOn ? "ON" : "OFF"}');
  }
  
  // ìŠ¤í”¼ì»¤ ìƒíƒœ
  bool get isSpeakerOn => _isSpeakerOn;

  // ì •ë¦¬
  Future<void> dispose() async {
    await _localStream?.dispose();
    await _remoteStream?.dispose();
    await _peerConnection?.close();
    await localRenderer.dispose();
    await remoteRenderer.dispose();
    
    _localStream = null;
    _remoteStream = null;
    _peerConnection = null;
    _isInitialized = false;
  }

  // í†µí™” ì¢…ë£Œ
  Future<void> endCall() async {
    await _localStream?.dispose();
    await _remoteStream?.dispose();
    await _peerConnection?.close();
    
    _localStream = null;
    _remoteStream = null;
    _peerConnection = null;
    
    localRenderer.srcObject = null;
    remoteRenderer.srcObject = null;
  }
}
